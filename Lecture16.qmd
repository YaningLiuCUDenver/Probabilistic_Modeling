---
title: Brownian Motion
toc: true
number-sections: true
format:
  html:
    code-fold: false
jupyter: python3
execute:
  enabled: false
bibliography: references.bib
---

## Brownian Motion

Let us start by considering the symmetric random walk, which in each time unit is equally likely to take a unit step either to the left or to the right. That is, it is a Markov chain with $P_{i,i+1}=\frac{1}{2}=P_{i,i-1},i=0, \pm 1, \ldots$. Now suppose that we speed up this process by taking smaller and smaller steps in smaller and smaller time intervals. If we now go to the limit in the right manner, what we obtain is Brownian motion.

More precisely, suppose that each $\Delta t$ time unit we take a step of size $\Delta x$ either to the left or the right with equal probabilities. If we let $X(t)$ denote the position at time $t$ then
$$
X(t)=\Delta x(X_1+\cdots+X_{[t/\Delta t]})
$$ {#eq-eq10-1}

where
$$
X_i=
\begin{cases}
+1, & \text{if the }i\text{th step of length }\Delta x \text{ is to the right},\\
-1, & \text{if it is to the left}
\end{cases}
$$
and $[t/\Delta t]$ is the largest integer less than or equal to $t/\Delta t$, and where the $X_i$ are
assumed independent with
$$
P\{X_i=1\}=P\{X_i=-1\}=\frac{1}{2}
$$
As $E[X_i]=0$, Var$(X_i)=E[X_i^2]=1$, we see from @eq-eq10-1 that
$$
\begin{align*}
E[X(t)] &= 0 \\
\mathrm{Var}(X(t))&=(\Delta x)^{2}\biggl[\frac{t}{\Delta t}\biggr]
\end{align*}
$$ {#eq-eq10-2}

We shall now let $\Delta x$ and $\Delta t$ go to 0. However, we must do it in a way such that the resulting limiting process is nontrivial (for instance, if we let $\Delta x=\Delta t$ and let $\Delta t\to 0$, then from the preceding we see that $E[X(t)]$ and $\mathrm{Var}(X(t))$ would both converge to 0 and thus $X(t)$ would equal 0 with probability 1). If we let $\Delta x=\sigma\sqrt{\Delta t}$ for some positive constant $\sigma$ then from @eq-eq10-2 we see that $as~\Delta t\to 0$
$$
E[X(t)]=0,\\
\mathrm{Var}(X(t))\to\sigma^{2}t
$$

We now list some intuitive properties of this limiting process obtained by taking $\Delta x=\sigma\sqrt{\Delta t}$ and then letting $\Delta t\to0.$ From @eq-eq10-1 and the central limit theorem the following seems reasonable:

(i) $X(t)$ is normal with mean 0 and variance $\sigma^2t.$ In addition, because the
changes of value of the random walk in nonoverlapping time intervals are
independent, we have

(ii) $\{X(t),t\geqslant0\}$ has independent increments, in that for all $t_1<t_2<\cdots<t_n$
$$
X(t_{n})-X(t_{n-1}),X(t_{n-1})-X(t_{n-2}),\ldots,X(t_{2})-X(t_{1}),X(t_{1})
$$
are independent. Finally, because the distribution of the change in position of the random walk over any time interval depends only on the length of that interval, it would appear that

(iii) $\{X( t)$, $t\geqslant 0\}$ has stationary increments, in that the distribution of
$X(t+s)-X(t)$ does not depend on $t.$ We are now ready for the following formal definition.

:::{#def-def1}
A stochastic process $\{X(t),t\geqslant0\}$ is said to be a *Brownian motion* process if

(i) $X(0)=0;$

(ii) $\{X(t),t\geqslant0\}$ has stationary and independent increments;

(iii) for every $t>0,X(t)$ is normally distributed with mean 0 and variance $\sigma^2t.$
:::

The Brownian motion process, sometimes called the Wiener process, is one of the most useful stochastic processes in applied probability theory. It originated in physics as a description of Brownian motion. This phenomenon, named after the English botanist Robert Brown who discovered it, is the motion exhibited by a small particle which is totally immersed in a liquid or gas. Since then, the process has been used beneficially in such areas as statistical testing of goodness of fit, analyzing the price levels on the stock market, and quantum mechanics.

The first explanation of the phenomenon of Brownian motion was given by Einstein in 1905. He showed that Brownian motion could be explained by assuming that the immersed particle was continually being subjected to bombardment by the molecules of the surrounding medium. However, the preceding concise definition of this stochastic process underlying Brownian motion was given by Wiener in a series of papers originating in 1918.

When $\sigma=1$, the process is called *standard Brownian motion*. Because any Brownian motion can be converted to the standard process by letting $B(t)=$ $X(t)/\sigma$ we shall, unless otherwise stated, suppose throughout this chapter that $\sigma=1.$

The interpretation of Brownian motion as the limit of the random walks [@eq-eq10-1] suggests that $X(t)$ should be a continuous function of $t.$ This turns out to be the case, and it may be proven that, with probability 1, $X(t)$ is indeed a continuous function of $t.$ This fact is quite deep, and no proof shall be attempted.

As $X(t)$ is normal with mean 0 and variance $t$, its density function is given by
$$
f_t(x)=\frac{1}{\sqrt{2\pi t}}e^{-x^2/2t}
$$
To obtain the joint density function of $X(t_1),X(t_2),\ldots,X(t_n)$ for $t_1<\cdots<t_n$,
note first that the set of equalities 
$$
\begin{align*}
X(t_1) &= x_1, \\
X(t_2) &= x_2, \\
&\vdots \\
X(t_n) &= x_n
\end{align*}
$$
is equivalent to
$$
\begin{align*}
X(t_1) &= x_1, \\
X(t_2) - X(t_1) &= x_2 - x_1, \\
&\vdots \\
X(t_n) - X(t_{n-1}) &= x_n - x_{n-1}
\end{align*}
$$

However, by the independent increment assumption it follows that $X(t_1)$, $X(t_2)-X(t_1), \ldots, X(t_n)-X(t_{n-1})$, are independent and, by the stationary increment assumption, that $X(t_k)-X(t_{k-1})$ is normal with mean 0 and variance $t_k-t_{k-1}$. Hence, the joint density of $X(t_1),\ldots,X(t_n)$ is given by
$$
\begin{align*}
&f(x_{1},x_{2},\ldots,x_{n})=f_{t_{1}}(x_{1})f_{t_{2}-t_{1}}(x_{2}-x_{1})\cdots f_{t_{n}-t_{n-1}}(x_{n}-x_{n-1})\\
&=\frac{\exp\left\{-\frac{1}{2}\biggl[\frac{x_{1}^{2}}{t_{1}}+\frac{(x_{2}-x_{1})^{2}}{t_{2}-t_{1}}+\cdots+\frac{(x_{n}-x_{n-1})^{2}}{t_{n}-t_{n-1}}\biggr]\right\}}{(2\pi)^{n/2}[t_{1}(t_{2}-t_{1})\cdots(t_{n}-t_{n-1})]^{1/2}}
\end{align*}
$$ {#eq-eq10-3}
From this equation, we can compute in principle any desired probabilities. For instance, suppose we require the conditional distribution of $X(s)$ given that $X(t)=B$ where $s<t.$ The conditional density is
$$
\begin{align*}
f_{s|t}(x|B)&=\frac{f_{s}(x)\:f_{t-s}(B-x)}{f_{t}(B)}\\
&=K_{1}\exp\{-x^{2}/2s-(B-x)^{2}/2(t-s)\}\\
&=K_{2}\exp\biggl\{-x^{2}\biggl(\frac{1}{2s}+\frac{1}{2(t-s)}\biggr)+\frac{Bx}{t-s}\biggr\}\\
&=K_{2}\exp\biggl\{-\frac{t}{2s(t-s)}\biggl(x^{2}-2\frac{sB}{t}x\biggr)\biggr\}\\
&=K_{3}\exp\biggl\{-\frac{(x-Bs/t)^{2}}{2s(t-s)/t}\biggr\}
\end{align*}
$$
where $K_1,K_2$, and $K_3$ do not depend on $x.$ Hence, we see from the preceding that the conditional distribution of $X(s)$ given that $X(t)=B$ is, for $s<t$, normal with mean and variance given by
$$
\begin{align*}
E[X(s)|X(t)=B]&=\frac{s}{t}B,\\
\mathrm{Var}[X(s)|X(t)=B]&=\frac{s}{t}(t-s)
\end{align*}
$$ {#eq-eq10-4}

:::{#exm-exa1}
In a bicycle race between two competitors, let $Y(t)$ denote the amount of time (in seconds) by which the racer that started in the inside position is ahead when 100t percent of the race has been completed, and suppose that $\{Y(t),~0\leqslant t\leqslant1\}$ can be effectively modeled as a Brownian motion process with variance parameter $\sigma^2.$

(a) If the inside racer is leading by $\sigma$ seconds at the midpoint of the race, what
is the probability that she is the winner?

(b) If the inside racer wins the race by a margin of $\sigma$ seconds, what is the
probability that she was ahead at the midpoint?
:::

:::{#sol-sol1}
(a)
$$
\begin{align*}
&P\{Y(1)>0|Y(1/2)=\sigma\} \\
&=P\{Y(1)-Y(1/2)>-\sigma|Y(1/2)=\sigma\}\\
&= P\{ Y( 1) - Y( 1/ 2) > - \sigma \}\quad \text{by independent increments} \\
&=P\{Y(1/2)>-\sigma\} \quad \text{by stationary increments}\\
&=P\left\{\frac{Y(1/2)}{\sigma/\sqrt{2}}>-\sqrt{2}\right\}\\
&=\Phi(\sqrt{2})\\
&\approx0.9213
\end{align*}
$$
where $\Phi(x)=P\{N(0,1)\leqslant x\}$ is the standard normal distribution function. 

(b) Because we must compute $P\{Y(1/2)>0|Y(1)=\sigma\}$, let us first determine the conditional distribution of $Y(s)$ given that $Y(t)=C$, when $s<t.$ Now, since $\{X(t),t\geqslant0\}$ is standard Brownian motion when $X(t)=Y(t)/\sigma$, we obtain from @eq-eq10-4 that the conditional distribution of $X(s)$, given that $X(t)=C/\sigma$, is normal with mean $sC/t\sigma$ and variance $s(t-s)/t.$ Hence, the conditional distribution of $Y(s)=\sigma X(s)$ given that $Y(t)=C$ is normal with mean $sC/t$ and variance $\sigma^2s(t-s)/t.$ Hence,
$$
\begin{align*}
P\{Y(1/2)>0|Y(1)=\sigma\}&=P\{N(\sigma/2,\sigma^2/4)>0\}\\
&=\Phi(1)\\
&\approx0.8413
\end{align*}
$$
:::

## Hitting Times, Maximum Variable, and the Gamblerâ€™s Ruin Problem

Let $T_a$ denote the first time the Brownian motion process hits $a.$ When $a>0$ we will compute $P\{T_a\leqslant t\}$ by considering $P\{X(t)\geqslant a\}$ and conditioning on whether or not $T_a\leqslant t.$ This gives
$$
\begin{align*}
P\{X(t)\geqslant a\}&=P\{X(t)\geqslant a|T_a\leqslant t\}P\{T_a\leqslant t\} \\
&+P\{X(t)\geqslant a|T_{a}>t\}P\{T_{a}>t\}
\end{align*}
$$ {#eq-eq10-5}

Now if $T_a\leqslant t$, then the process hits $a$ at some point in $[0,t]$ and, by symmetry, it
is just as likely to be above $a$ or below $a$ at time $t.$ That is
$$
P\{X(t)\geqslant a|T_a\leqslant t\}=\frac{1}{2}
$$

As the second right-hand term of @eq-eq10-5 is clearly equal to 0 (since, by continuity, the process value cannot be greater than $a$ without having yet hit $a)$, we see that
$$
\begin{align*}
P\{T_{a}\leqslant t\}&=2P\{X(t)\geqslant a\}\\
&=\frac{2}{\sqrt{2\pi t}}\int_{a}^{\infty}e^{-x^{2}/2t}\:dx\\
&=\frac{2}{\sqrt{2\pi}}\int_{a/\sqrt{t}}^{\infty}e^{-y^{2}/2}\:dy,\quad a>0
\end{align*}
$$ {#eq-eq10-6}

For $a<0$, the distribution of $T_a$ is, by symmetry, the same as that of $T_{-a}.$
Hence, from @eq-eq10-6 we obtain
$$
P\{T_{a}\leqslant t\}=\frac{2}{\sqrt{2\pi}}\int_{|a|/\sqrt{t}}^{\infty}e^{-y^{2}/2}\:dy
$$ {#eq-eq-10-7}

Another random variable of interest is the maximum value the process attains
in $[0,t].$ Its distribution is obtained as follows: For $a>0$
$$
\begin{align*}
P\left\{\max_{0\leqslant s\leqslant t}X(s)\geqslant a\right\}&=P\{T_{a}\leqslant t\}\quad\mathrm{by~continuity}\\
&=2P\{X(t)\geqslant a\}\quad\mathrm{from~} (6)\\
&=\frac{2}{\sqrt{2\pi}}\int_{a/\sqrt{t}}^{\infty}e^{-y^{2}/2}\:dy
\end{align*}
$$
Let us now consider the probability that Brownian motion hits A before $-B$ where $A>0,B>0.$ To compute this we shall make use of the interpretation of Brownian motion as being a limit of the symmetric random walk. To start let us recall from the results of the gambler's ruin problem that the probability that the symmetric random walk goes up $A$ before going down $B$ when each step is equally likely to be either up or down a distance $\Delta x$ is with $N= (A+ B) / \Delta x, i= B/ \Delta x]$ equal to $B\Delta x/(A+B)\Delta x=$ $B/(A+B).$

Hence, upon letting $\Delta x\to0$, we see that
$$
P\{\text{up }A\text{ before down }B\}=\frac{B}{A+B}
$$

