---
title: Introduction to Monte Carlo Simulation
toc: true
number-sections: true
format:
  html:
    code-fold: false
jupyter: python3
execute:
  enabled: false
bibliography: references.bib
---

Let $\mathbf{X}=(X_1,\ldots,X_n)$ denote a random vector having a given density function $f(x_1,\ldots,x_n)$ and suppose we are interested in computing
$$
E[g(\mathbf{X})]=\int\int\cdots\int g(x_1,\ldots,x_n)f(x_1,\ldots,x_n)\:dx_1\:dx_2\cdots dx_n
$$
for some $n$-dimensional function $g$. In many situations, it is not analytically possible either to compute the preceding multiple integral exactly or even to numerically approximate it within a given accuracy. One possibility that remains is to approximate $E[g(\mathbf{X})]$ by means of simulation.

To approximate $E[g(\mathbf{X})]$, start by generating a random vector $\mathbf{X}^{(1)}=(X_1^{(1)},\ldots, X_n^{(1)})$, having the joint density $f(x_1,\ldots,x_n)$ and then compute $Y^{(1)}=g(\mathbf{X}^{(1)}).$ Now generate a second random vector (independent of the first) $\mathbf{X}^{(2)}$ and compute $Y^{(2)}=g(\mathbf{X}^{(2)}).$ Keep on doing this until $r$, a fixed number, of independent and identically distributed random variables $Y^{(i)}=g(\mathbf{X}^{(i)}),i=1,\ldots,r$ have been generated. Now by the strong law of large numbers, we know that
$$
\lim_{r\to\infty}\frac{Y^{(1)}+\cdots+Y^{(r)}}{r}=E[Y^{(i)}]=E[g(\mathbf{X})]
$$
and so we can use the average of the generated $Y$s as an estimate of $E[g(\mathbf{X})].$ This approach to estimating $E[g(\mathbf{X})]$ is called the *Monte Carlo simulation* approach. Clearly there remains the problem of how to generate, or *simulate*, random vectors having a specified joint distribution. The first step in doing this is to be able to generate random variables from a uniform distribution on $(0, 1)$. One way to do this would be to take $10$ identical slips of paper, numbered $0, 1, \dots, 9$, place
them in a hat and then successively select $n$ slips, with replacement, from the hat.
The sequence of digits obtained (with a decimal point in front) can be regarded
as the value of a uniform $(0, 1)$ random variable rounded off to the nearest $\left(\frac{1}{10}\right)^n$.
For instance, if the sequence of digits selected is $3, 8, 7, 2, 1$, then the value of the
uniform $(0, 1)$ random variable is $0.38721$ (to the nearest $0.00001$). Tables of the
values of uniform $(0, 1)$ random variables, known as random number tables, have
been extensively published [for instance, see The RAND Corporation, *A Million
Random Digits with $100,000$ Normal Deviates* (New York: The Free Press, 1955)].

However, this is not the way in which digital computers simulate uniform $(0, 1)$
random variables. In practice, they use pseudo random numbers instead of truly
random ones. Most random number generators start with an initial value $X_0$,
called the seed, and then recursively compute values by specifying positive integers
$a, c$, and $m$, and then letting
$$
X_{n+1} = (aX_n+c) \text{ modulo } m, \quad n \ge 0
$$
where the preceding means that $aX_n + c$ is divided by $m$ and the remainder is
taken as the value of $X_{n+1}$. Thus each $X_n$ is either $0, 1, \dots, m−1$ and the quantity
$X_n/m$ is taken as an approximation to a uniform $(0, 1)$ random variable. It can be
shown that subject to suitable choices for $a, c, m$, the preceding gives rise to a
sequence of numbers that looks as if it were generated from independent uniform
$(0, 1)$ random variables.

As our starting point in the simulation of random variables from an arbitrary
distribution, we shall suppose that we can simulate from the uniform $(0, 1)$ distribution,
and we shall use the term "random numbers" to mean independent random
variables from this distribution. Then we will discuss how to simulate general continuous random variables,
discrete random variables, jointly distributed random variables, and stochastic
processes. Let us first consider two applications of simulation to combinatorial problems.

:::{#exm-exa1}
(Generating a Random Permutation) Suppose we are interested
in generating a permutation of the numbers $1, 2, \dots, n$ that is such that all
$n!$ possible orderings are equally likely. The following algorithm will accomplish
this by first choosing one of the numbers $1, \dots, n$ at random and then putting that
number in position $n$; it then chooses at random one of the remaining $n-1$ numbers
and puts that number in position $n - 1$; it then chooses at random one of the
remaining $n-2$ numbers and puts it in position $n-2$, and so on (where choosing
a number at random means that each of the remaining numbers is equally likely 
to be chosen). However, so that we do not have to consider exactly which of the
numbers remain to be positioned, it is convenient and efficient to keep the numbers
in an ordered list and then randomly choose the position of the number rather
than the number itself. That is, starting with any initial ordering $p_1, p_2, \dots, p_n$,
we pick one of the positions $1, \dots, n$ at random and then interchange the number
in that position with the one in position n. Now we randomly choose one of the
positions $1, \dots, n−1$ and interchange the number in this position with the one in
position $n-1$, and so on.

To implement the preceding, we need to be able to generate a random variable
that is equally likely to take on any of the values $1, 2, \dots, k$. To accomplish this,
let $U$ denote a random number—that is, $U$ is uniformly distributed over $(0, 1)$—
and note that $kU$ is uniform on $(0, k)$ and so
$$
P\{i-1 < kU < i\} = \frac{1}{k}, \quad i=1,\dots,k
$$
Hence, if the random variable $I = [kU]+1$ will be such that
$$
P\{I = i\} = P\{[kU] = i −1\} = P\{i −1 < kU <i\} = \frac{1}{k}
$$
The preceding algorithm for generating a random permutation can now be written
as follows:

Step 1: Let $p_1, p_2, \dots, p_n$ be any permutation of $1, 2, \dots, n$ (for instance, we can choose $p_j = j, j = 1, \dots, n)$.

Step 2: Set $k = n$.

Step 3: Generate a random number $U$ and let $I = [kU]+1$.

Step 4: Interchange the values of $p_I$ and $p_k$.

Step 5: Let $k = k - 1$ and if $k >1$ go to Step 3.

Step 6: $p_1, \dots, p_n$ is the desired random permutation

For instance, suppose $n = 4$ and the initial permutation is $1, 2, 3, 4$. If the first
value of $I$ (which is equally likely to be either $1, 2, 3, 4$) is $I = 3$, then the new
permutation is $1, 2, 4, 3$. If the next value of $I$ is $I = 2$ then the new permutation
is $1, 4, 2, 3$. If the final value of $I$ is $I = 2$, then the final permutation is $1, 4, 2, 3$,
and this is the value of the random permutation.

One very important property of the preceding algorithm is that it can also be
used to generate a random subset, say of size $r$, of the integers $1, \dots, n$. Namely,
just follow the algorithm until the positions $n, n−1, \dots, n−r +1$ are filled. The
elements in these positions constitute the random subset.
:::

The algorithm can be realized with Python as follows.

```{python}
import numpy as np

def int_perm(n):
    """
    Generate a permutation of integers 1,2,...,n

    Input:
    n: int, the largest integers

    Output:
    perm: 1d array of size n, the array of permuted integers of 1,2,...,n
    """

    perm = np.arange(1, n+1, dtype='int')
    k = n
    while k > 1:
        U = np.random.random()
        I = int(k*U)  # No need to add 1 since Python indices start from 0
        perm[[I, k-1]] = perm[[k-1, I]]  # exchange the int at pos I and pos k-1, noting k starts with n, hence k-1
        k -= 1
    return perm
```

```{python}
n = 10
print('A permutation of integers from ', 1 , ' to ', n, ' is: ', int_perm(n))
print('Another permutation of integers from ', 1 , ' to ', n, ' is: ', int_perm(n))
```

:::{#exm-exa2}
(Estimating the Number of Distinct Entries in a Large List) Consider a list of $n$ entries where $n$ is very large, and suppose we are interested in estimating $d$, the number of distinct elements in the list. If we let $m_i$ denote the
number of times that the element in position $i$ appears on the list, then we can
express $d$ by
$$
d=\sum_{i=1}^n\frac{1}{m_i}
$$
To estimate $d$, suppose that we generate a random value $X$ equally likely to be either $1,2,\ldots,n$ (that is, we take $X=[nU]+1)$ and then let $m(X)$ denote the number of times the element in position $X$ appears on the list. Then
$$
E\biggl[\dfrac{1}{m(X)}\biggr]=\sum_{i=1}^n\dfrac{1}{m_i}\dfrac{1}{n}=\dfrac{d}{n}
$$
Hence, if we generate $k$ such random variables $X_1,\ldots,X_k$ we can estimate $d$ by
$$
d\approx\frac{n\sum_{i=1}^{k}1/m(X_{i})}{k}
$$
Suppose now that each item in the list has a value attached to it—$v(i)$ being the value of the $i$th element. The sum of the values of the distinct items—call it $v$— can be expressed as
$$
v=\sum_{i=1}^n\frac{v(i)}{m(i)}
$$
Now if $X=[nU]+1$, where $U$ is a random number, then
$$
E\biggl[\frac{v(X)}{m(X)}\biggr]=\sum_{i=1}^{n}\frac{v(i)}{m(i)}\frac{1}{n}=\frac{v}{n}
$$
Hence, we can estimate $v$ by generating $X_1,\ldots,X_k$ and then estimating $v$ by
$$
v\approx\frac{n}{k}\sum_{i=1}^k\frac{v(X_i)}{m(X_i)}
$$
:::

