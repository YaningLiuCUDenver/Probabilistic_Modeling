---
title: General Techniques for Simulating Continuous Random Variables
toc: true
number-sections: true
format:
  html:
    code-fold: false
jupyter: python3
execute:
  enabled: false
bibliography: references.bib
---

## The Inverse Transformation Method

A general method for simulating a random variable having a continuous distribution—
called the *inverse transformation method*—is based on the following proposition.

:::{#prp-prp1}
Let $U$ be a uniform $(0, 1)$ random variable. For any continuous
distribution function $F$ if we define the random variable $X$ by
$$
X = F^{-1}(U)
$$
then the random variable $X$ has distribution function $F$. [$F^{-1}(u)$ is defined to
equal that value $x$ for which $F(x) = u$.]
:::

:::{.proof}
$$
F_X(a) = P\{X\le a\} = P\{F^{-1}(U) \le a\}
$$ {#eq-eq11-1}

Now, since $F(x)$ is a monotone function, it follows that $F^{−1}(U) \le a$ if and only
if $U \le F(a)$. Hence, from @eq-eq11-1, we see that
$$
F_X(a) = P\{U\le F(a)\} = F(a)
$$
:::

Hence we can simulate a random variable $X$ from the continuous distribution $F$, when $F^{−1}$ is computable, by simulating a random number $U$ and then setting $X = F^{−1}(U)$.

:::{#exm-exa1}
(Simulating an Exponential Random Variable) If $F(x)=1-e^{-x}$,then $F^{-1}(u)$ is that value of $x$ such that
$$
1-e^{-x}=u
$$
or
$$
x=-\log{(1-u)}
$$
Hence, if $U$ is a uniform $(0,1)$ variable, then
$$
F^{-1}(U)=-\log{(1-U)}
$$
is exponentially distributed with mean 1. Since $1-U$ is also uniformly distributed on $(0,1)$ it follows that $-\log U$ is exponential with mean 1. Since $cX$ is exponential with mean $c$ when $X$ is exponential with mean 1, it follows that $-c\log U$ is exponential with mean $c$.
:::

```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import expon

def exp_invtran(sample_size):
    """
    Simulate an array of exponential variables (lambda=1) using inverse transform
    
    Input
    sample_size: int, number of exponential variables to generate

    Ouput
    exp_rvs: 1d array of shape (sample_size, ), the array of generated exponential variables
    """
    
    U = np.random.rand(sample_size)
    exp_rvs = -np.log(1-U)

    return exp_rvs
```

```{python}
sample_size = 10000
exp_rvs = exp_invtran(sample_size)

# density plot for the samples
plt.hist(exp_rvs, bins=50, density=True, color='b', label='hist')

# theoretical pdf
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = expon.pdf(x)
plt.plot(x, p, 'r', linewidth=2, label='density')

plt.xlabel('x')
plt.ylabel('Density')
plt.title('Histogram and PDF of Exponential Distribution')
plt.legend()

plt.show()
```

## The Rejection Method

Suppose that we have a method for simulating a random variable having density function $g(x)$. We can use this as the basis for simulating from the continuous distribution having density $f(x)$ by simulating $Y$ from $g$ and then accepting this simulated value with a probability proportional to $f(Y)/g(Y)$.
Specifically let $c$ be a constant such that
$$
\frac{f(y)}{g(y)}\leqslant c\quad\mathrm{for~all~y}
$$
We then have the following technique for simulating a random variable having
density $f$.

**Rejection Method**

Step 1: Simulate $Y$ having density $g$ and simulate a random number $U.$

Step 2: If $U\leqslant f(Y)/cg(Y)$ set $X=Y.$ Otherwise return to Step 1.

:::{#prp-prp2}
The random variable $X$ generated by the rejection method
has density function $f$.
:::

:::{.proof}
Let $X$ be the value obtained, and let $N$ denote the number of necessary
iterations. Then
$$
\begin{align*}
P\{X\leqslant x\}&=P\{Y_{N}\leqslant x\}\\
&=P\{Y\leqslant x|U\leqslant f(Y)/cg(Y)\}\\
&=\frac{P\{Y\leqslant x,U\leqslant f(Y)/cg(Y)\}}{K}\\
&=\frac{\int P\{Y\leqslant x,U\leqslant f(Y)/cg(Y)|Y=y\}g(y)\:dy}{K}\\
&=\frac{\int_{-\infty}^{x}(f(y)/cg(y))g(y)\:dy}{K}\\
&=\frac{\int_{-\infty}^{x}f(y)\:dy}{Kc}
\end{align*}
$$
where $K= P\{ U\leqslant f( Y) / cg( Y) \}$. Letting $x\to \infty$ shows that $K= 1/ c$ and the proof is complete.
:::

:::{#exm-exa2}
Let us use the rejection method to generate a random variable
having density function
$$
f(x)=20x(1-x)^3,\quad0<x<1
$$
Since this random variable (which is beta with parameters $2,4$) is concentrated in
the interval $(0,1)$, let us consider the rejection method with
$$
g(x)=1,\quad 0<x<1
$$
To determine the constant $c$ such that $f(x)/g(x)\leqslant c$, we use calculus to determine the maximum value of
$$
\frac{f(x)}{g(x)}=20x(1-x)^3
$$
Differentiation of this quantity yields
$$
\dfrac{d}{dx}\biggl[\dfrac{f(x)}{g(x)}\biggr]=20[(1-x)^3-3x(1-x)^2]
$$
Setting this equal to 0 shows that the maximal value is attained when $x=\frac{1}{4}$, and
thus
$$\frac{f(x)}{g(x)}\leqslant20\biggl(\frac14\biggr)\biggl(\frac34\biggr)^3=\frac{135}{64}\equiv c$$
Hence,
$$
\frac{f(x)}{cg(x)}=\frac{256}{27}\:x(1-x)^3
$$
and thus the rejection procedure is as follows:

Step 1: Generate random numbers $U_1$ and $U_2$.

Step 2: If $U_{2}\leqslant\frac{256}{27}U_{1}(1-U_{1})^{3}$, stop and set $X=U_1.$ Otherwise return to
Step 1.

The average number of times that step 1 will be performed is $c= \frac {135}{64}.$

```{python}
sample_size = 10000  # number of samples to generate
count = 0
samples = np.zeros(sample_size)
while count < sample_size:
    U1 = np.random.random()  # generate a sample from distribution g, in this case, uniform U(0,1)
    U2 = np.random.random()
    if U2 <= 256/27*U1*(1-U1)**3:
        samples[count] = U1
        count +=1
```

```{python}
from scipy.stats import beta

# density plot for the samples
plt.hist(samples, bins=50, density=True, color='b', label='hist')

# theoretical pdf
a = 2
b = 4
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = beta.pdf(x, a, b)
plt.plot(x, p, 'r', linewidth=2, label='density')

plt.xlabel('x')
plt.ylabel('Density')
plt.title('Histogram and PDF of beta(2,4) Distribution')
plt.legend()

plt.show()
```

:::{#exm-exa3}
(Simulating a Normal Random Variable) To simulate a standard normal random variable $Z$ (that is, one with mean $0$ and variance $1$) note first that the absolute value of $Z$ has density function
$$
f(x)=\frac{2}{\sqrt{2\pi}}e^{-x^{2}/2},\quad0<x<\infty
$$ {#eq-eq11-2}


We will start by simulating from the preceding density by using the rejection
method with
$$
g(x)=e^{-x},\quad0<x<\infty
$$
Now, note that
$$
\frac{f(x)}{g(x)}=\sqrt{2e/\pi}\exp\{-(x-1)^{2}/2\}\leqslant\sqrt{2e/\pi}
$$
Hence, using the rejection method we can simulate from @eq-eq11-2 as follows:

(a) Generate independent random variables $Y$ and $U, Y$ being exponential with rate 1 and $U$ being uniform on (0,1).

(b) If $U\leqslant\exp\{-(Y-1)^2/2\}$, or equivalently, if

$$
-\log U\geqslant(Y-1)^{2}/2
$$
set $X=Y.$ Otherwise return to step (a).

Once we have simulated a random variable $X$ having density function @eq-eq11-2 we can then generate a standard normal random variable $Z$ by letting $Z$ be equally likely to be either $X$ or $-X.$

To improve upon the foregoing, note first that from @exm-exa1 it follows that $-\log U$ will also be exponential with rate 1. Hence, steps (a) and (b) are equivalent to the following:

(a') Generate independent exponentials with rate $1$, $Y_1$, and $Y_2$. 

(b') Set $X=Y_1$ if $Y_2\geqslant(Y_1-1)^2/2.$ Otherwise return to (a'). 

Now suppose that we accept step (b'). It then follows by the lack of memory property of the exponential that the amount by which $Y_2$ exceeds $(Y_1-1)^2/2$ will also be exponential with rate 1.

Hence, summing up, we have the following algorithm which generates an exponential with rate 1 and an independent standard normal random variable.

Step 1: Generate $Y_{1}$, an exponential random variable with rate 1.

Step 2: Generate $Y_2$, an exponential with rate 1.

Step 3: If $Y_2-(Y_1-1)^2/2>0$, set $Y=Y_2-(Y_1-1)^2/2$ and go to step 4. Otherwise go to step 1.

Step 4: Generate a random number $U$ and set
$$
Z=\left\{
    \begin{matrix}Y_1,&\quad\text{if }U\leqslant\frac{1}{2}\\
    -Y_1,&\quad\text{if }U>\frac{1}{2}
    \end{matrix}
    \right.
$$
The random variables $Z$ and $Y$ generated by the preceding are independent with
$Z$ being normal with mean $0$ and variance $1$ and $Y$ being exponential with rate $1$.
(If we want the normal random variable to have mean $\mu$ and variance $\sigma^2$, just take
$\mu+\sigma z$.)
:::

