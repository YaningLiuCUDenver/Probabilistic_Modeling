---
title: Special Techniques for Simulating Continuous Random Variables
toc: true
number-sections: true
format:
  html:
    code-fold: false
jupyter: python3
execute:
  enabled: false
bibliography: references.bib
---

Special techniques have been devised to simulate from most of the common continuous
distributions. We now present some of these.

## The Normal Distribution (Box-Muller Method)

Let $X$ and $Y$ denote independent standard normal random variables and thus have
the joint density function
$$
f(x,y)=\frac{1}{2\pi}e^{-(x^{2}+y^{2})/2},\quad-\infty<x<\infty,-\infty<y<\infty
$$

![An elephant](images/box_muller.png){#fig-fig1}

Consider now the polar coordinates of the point $(X,Y).$  As shown in @fig-fig1,
$$
\begin{aligned}
R^{2}&=X^{2}+Y^{2},\\
\Theta&=\tan^{-1}Y/X
\end{aligned}
$$
To obtain the joint density of $R^2$ and $\Theta$, consider the transformation
$$
d=x^2+y^2,\quad\theta=\tan^{-1}(y/x)
$$

The Jacobian of this transformation is
$$
\begin{aligned}
\text{J}&=
\begin{vmatrix}\dfrac{\partial d}{\partial x}&\dfrac{\partial d}{\partial y}\\\dfrac{\partial\theta}{\partial x}&\dfrac{\partial\theta}{\partial y}\end{vmatrix}=\begin{vmatrix}2x&2y\\\dfrac{1}{1+y^2/x^2}\biggl(\dfrac{-y}{x^2}\biggr)&\dfrac{1}{1+y^2/x^2}\biggl(\dfrac{1}{x}\biggr)\end{vmatrix}\\&=2\begin{vmatrix}x&y\\-\dfrac{y}{x^2+y^2}&\dfrac{x}{x^2+y^2}
\end{vmatrix}
=2
\end{aligned}
$$
Hence, the joint density of $R^{2}$ and $\Theta$ is given by
$$
\begin{aligned}
f_{R^{2},\Theta}(d,\theta)&=\frac{1}{2\pi}e^{-d/2}\frac{1}{2}\\
&=\frac{1}{2}e^{-d/2}\frac{1}{2\pi},\quad 0<d<\infty, 0<\theta<2\pi
\end{aligned}
$$
Thus, we can conclude that $R^2$ and $\Theta$ are independent with $R^2$ having an exponential distribution with rate $\frac{1}{2}$ and $\Theta$ being uniform on $(0,2\pi).$

Let us now go in reverse from the polar to the rectangular coordinates. From the preceding if we start with $W$, an exponential random variable with rate $\frac{1}{2}$ ($W$ plays the role of $R^2$) and with $V$, independent of $W$ and uniformly distributed over $(0,2\pi)$ ($V$ plays the role of $\Theta$) then $X=\sqrt{W}\cos V, Y=\sqrt{W}\sin V$ will be independent standard normals. Hence using the results of Example 1 Lecture 19 we see that if $U_1$ and $U_2$ are independent uniform $(0,1)$ random numbers, then
$$
\begin{matrix}
X=(-2\log\:U_1)^{1/2}\cos(2\pi U_2),\\
Y=(-2\log\:U_1)^{1/2}\sin(2\pi U_2)
\end{matrix}
$$ {#eq-eq11-4}
are independent standard normal random variables.

The preceding approach to generating standard normal random variables is called the *Box-Muller* approach. Its efficiency suffers somewhat from its need to compute the preceding sine and cosine values. There is, however, a way to get around this potentially time-consuming difficulty. To begin, note that if $U$ is uniform on $(0,1)$, then $2U$ is uniform on $(0,2)$, and so $2U-1$ is uniform on $(-1,1)$.

![](images/box_muller2.png){#fig-fig2}

Thus, if we generate random numbers $U_1$ and $U_2$ and set
$$
\begin{aligned}
V_1 &= 2U_1-1, \\
V_{2} &= 2U_{2}-1
\end{aligned}
$$
then $(V_1,V_2)$ is uniformly distributed in the square of area 4 centered at $(0,0)$ (see @fig-fig2).

Suppose now that we continually generate such pairs $(V_1,V_2)$ until we obtain one that is contained in the circle of radius 1 centered at $(0,0)$—that is, until $(V_1,V_2)$ is such that $V_1^2+V_2^2\leqslant1.$ It now follows that such a pair $(V_1,V_2)$ is uniformly distributed in the circle. If we let $\bar{R},\bar{\Theta}$ denote the polar coordinates of this pair, then it is easy to verify that $\bar{R}$ and $\bar{\Theta}$ are independent, with $\bar{R}^2$ being uniformly distributed on $(0,1)$, and $\bar{\Theta}$ uniformly distributed on $(0,2\pi).$

Since
$$
\begin{aligned}
\sin\bar{\Theta}&=V_{2}/\bar{R}=\frac{V_{2}}{\sqrt{V_{1}^{2}+V_{2}^{2}}},\\
\cos\bar{\Theta}&=V_{1}/\bar{R}=\frac{V_{1}}{\sqrt{V_{1}^{2}+V_{2}^{2}}}
\end{aligned}
$$
it follows from @eq-eq11-4 that we can generate independent standard normals $X$ and $Y$ by generating another random number $U$ and setting
$$
\begin{aligned}
X &= (-2\log U)^{1/2}V_{1}/\bar{R},\\ 
Y &= (-2\log U)^{1/2}V_{2}/\bar{R}
\end{aligned}
$$
In fact, since (conditional on $V_1^2+V_2^2\leqslant1$) $\bar{R}^2$ is uniform on $(0,1)$ and is independent of $\bar{\Theta}$, we can use it instead of generating a new random number $U;$ thus showing that
$$
\begin{aligned}
X&=(-2\log\bar{R}^{2})^{1/2}V_{1}/\bar{R}=\sqrt{\frac{-2\log S}{S}}V_{1},\\
Y&=(-2\log\bar{R}^{2})^{1/2}V_{2}/\bar{R}=\sqrt{\frac{-2\log S}{S}}V_{2}
\end{aligned}
$$
are independent standard normals, where
$$
S=\bar{R}^2=V_1^2+V_2^2
$$

Summing up, we thus have the following approach to generating a pair of independent standard normals:

Step 1: Generate random numbers $U_1$ and $U_2.$

Step 2: Set $V_1= 2U_{1}- 1$, $V_{2}= 2U_{2}- 1$, $S= V_{1}^{2}+ V_{2}^{2}.$

Step3: If $S>1$, return to step 1

Step 4: Return the independent unit normals
$$
X=\sqrt{\frac{-2\log S}{S}}V_1,\quad Y=\sqrt{\frac{-2\log S}{S}}V_2
$$
The preceding is called the *polar method*. Since the probability that a random point in the square will fall within the circle is equal to $\pi/4$ (the area of the circle divided by the area of the square), it follows that, on average, the polar method will require $4/\pi=1.273$ iterations of step 1. Hence, it will, on average, require $2.546$ random numbers, $1$ logarithm, $1$ square root, $1$ division, and $4.546$ multiplications to generate $2$ independent standard normals.

```{python}
import numpy as np

def box_muller(sample_size):
    """
    Generate standard normal variables with the box-muller method
    input:
    sample_size: int, the sample size (should be an even number)

    output:
    samples: 1d array of size sample_size, the generated standard normal variables
    """
    samples = np.zeros(sample_size)
    for i in range(int(sample_size/2)):
        (U1, U2) = np.random.rand(2)
        X1 = np.sqrt(-2*np.log(U1))*np.cos(2*np.pi*U2)
        X2 = np.sqrt(-2*np.log(U1))*np.sin(2*np.pi*U2)
        samples[2*i:2*i+1+1] = [X1, X2]
    return samples
```

```{python}
from scipy.stats import norm
import matplotlib.pyplot as plt

sample_size = 10000
samples = box_muller(sample_size)

# density plot for the samples
plt.hist(samples, bins=50, density=True, color='b', label='hist')

# theoretical pdf
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = norm.pdf(x)
plt.plot(x, p, 'r', linewidth=2, label='density')

plt.xlabel('x')
plt.ylabel('Density')
plt.title('Histogram and PDF of standard normal Distribution')
plt.legend()

plt.show()
```

## The Gamma Distribution

To simulate from a gamma distribution with parameters $(n,\lambda)$, where $n$ is an integer, we use the fact that the sum of $n$ independent exponential random variables each having rate $\lambda$ has this distribution. Hence, if $U_1,\ldots,U_n$ are independent uniform (0,1) random variables,
$$X=\dfrac{1}{\lambda}\sum_{i=1}^{n}\log\:U_{i}=-\dfrac{1}{\lambda}\:\log\biggl(\prod_{i=1}^{n}\:U_{i}\biggr)$$
has the desired distribution.

## The Chi-Squared Distribution

The chi-squared distribution with $n$ degrees of freedom is the distribution of $\chi_{n}^{2}=Z_{1}^{2}+\cdots+Z_{n}^{2}$ where $Z_i,i=1,\ldots,n$ are independent standard normals. Using the fact that $Z_1^2+Z_2^2$ has an exponential distribution with rate $\frac{1}{2}.$ Hence, when $n$ is even—say $n=2k$—$\chi_{2k}^{2}$ has a gamma distribution with parameters $(k,\frac{1}{2}).$ Hence, $-2\log(\prod_{i=1}^kU_i)$ has a chi-squared distribution with $2k$ degrees of freedom. We can simulate a chisquared random variable with $2k+1$ degrees of freedom by first simulating a standard normal random variable $Z$ and then adding $Z^2$ to the preceding. That is,
$$
\chi_{2k+1}^2=Z^2-2\log\biggl(\prod_{i=1}^k\:U_i\biggr)
$$
where $Z,U_1,\ldots,U_n$ are independent with $Z$ being a standard normal and the
others being uniform $(0,1)$ random variables.

