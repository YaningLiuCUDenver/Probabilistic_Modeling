---
title: Variance Reduction Techniques-Part I
toc: true
number-sections: true
format:
  html:
    code-fold: false
jupyter: python3
execute:
  enabled: false
bibliography: references.bib
---

Let $X_1,\ldots,X_n$ have a given joint distribution, and suppose we are interested in
computing
$$
\theta\equiv E[g(X_1,\ldots,X_n)]
$$
where $g$ is some specified function. It is often the case that it is not possible to analytically compute the preceding, and when such is the case we can attempt to use simulation to estimate $\theta.$ This is done as follows: Generate $X_1^{(1)},\ldots,X_n^{(1)}$ having the same joint distribution as $X_1,\ldots,X_n$ and set
$$
Y_1=g\bigl(X_1^{(1)},\ldots,X_n^{(1)}\bigr)
$$
Now, simulate a second set of random variables (independent of the first set)
$X_1^{(2)},\ldots,X_n^{(2)}$ having the distribution of $X_1,\ldots,X_n$ and set
$$
Y_2=g\bigl(X_1^{(2)},\ldots,X_n^{(2)}\bigr)
$$
Continue this until you have generated $k$ (some predetermined number) sets, and so have also computed $Y_1,Y_2,\ldots,Y_k.$ Now, $Y_1,\ldots,Y_k$ are independent and identically distributed random variables each having the same distribution of $g(X_1,\ldots,X_n).$ Thus, if we let $\bar{Y}$ denote the average of these $k$ random variables—that is, 
$$
\bar{Y}=\sum_{i=1}^kY_i/k
$$
then
$$
\begin{align*}
E[\bar{Y}]&=\theta, \\
E\Big[(\bar{Y}-\theta)^2\Big]&=\mathrm{Var}(\bar{Y})
\end{align*}
$$
Hence, we can use $\bar{Y}$ as an estimate of $\theta.$ As the expected square of the difference between $\bar{Y}$ and $\theta$ is equal to the variance of $\bar{Y}$, we would like this quantity to be as small as possible. [In the preceding situation, $\mathrm{Var}(\bar{Y}) = \mathrm{Var}(Y_i)/k$, usually not known in advance but must be estimated from the generated values $Y_1,\ldots,Y_n$.] We now present three general techniques for reducing the variance of our estimator.

### Use of Antithetic Variables

In the preceding situation, suppose that we have generated $Y_1$ and $Y_2$, identically
distributed random variables having mean $\theta.$ Now,
$$
\begin{align*}
\mathrm{Var}\biggl(\frac{Y_{1}+Y_{2}}{2}\biggr)&=\frac{1}{4}[\mathrm{Var}(Y_{1})+\mathrm{Var}(Y_{2})+2\mathrm{Cov}(Y_{1},Y_{2})]\\
&=\frac{\mathrm{Var}(Y_{1})}{2}+\frac{\mathrm{Cov}(Y_{1},Y_{2})}{2}
\end{align*}
$$
Hence, it would be advantageous (in the sense that the variance would be reduced) if $Y_1$ and $Y_2$ rather than being independent were negatively correlated. To see how we could arrange this, let us suppose that the random variables $X_1,\ldots,X_n$ are independent and, in addition, that each is simulated via the inverse transform technique. That is, $X_i$ is simulated from $F_i^{-1}(U_i)$ where $U_i$ is a random number and $F_i$ is the distribution of $X_i.$ Hence, $Y_1$ can be expressed as
$$
Y_{1}=g\big(F_{1}^{-1}(U_{1}),\ldots,F_{n}^{-1}(U_{n})\big)
$$
Now, since $1-U$ is also uniform over $(0,1)$ whenever $U$ is a random number
(and is negatively correlated with $U$) it follows that $Y_2$ defined by
$$
Y_{2}=g\big(F_{1}^{-1}(1-U_{1}),\ldots,F_{n}^{-1}(1-U_{n})\big)
$$
will have the same distribution as $Y_{1}.$ Hence, if $Y_{1}$ and $Y_{2}$ were negatively correlated, then generating $Y_2$ by this means would lead to a smaller variance than if it were generated by a new set of random numbers. (In addition, there is a computational savings since rather than having to generate $n$ additional random numbers, we need only subtract each of the previous $n$ from 1.) The following theorem will be the key to showing that this technique—known as the use of antithetic variables—will lead to a reduction in variance whenever $g$ is a monotone function. 

:::{#thm-thm1}
If $X_1,\ldots,X_n$ are independent, then, for any increasing functions $f$ and $g$ of $n$ variables,
$$
E[f(\mathbf{X})g(\mathbf{X})]\geqslant E[f(\mathbf{X})]E[g(\mathbf{X})]
$$ {#eq-eq11-11}
where $\mathbf{X}=(X_1,\ldots,X_n).$
:::

:::{.proof}
The proof is by induction on $n$. To prove it when $n=1$, let $f$ and g be
increasing functions of a single variable. Then, for any $x$ and $y$,
$$
(f(x)-f(y))(g(x)-g(y))\geqslant 0
$$
since if $x\geqslant y\left(x\leqslant y\right)$ then both factors are nonnegative (nonpositive). Hence, for
any random variables $X$ and $Y$,
$$
(f(X)-f(Y))(g(X)-g(Y))\geqslant 0
$$
implying that
$$
E[(f(X)-f(Y))(g(X)-g(Y))]\geqslant0
$$
or, equivalently,
$$
E[f(X)g(X)]+E[f(Y)g(Y)]\geqslant E[f(X)g(Y)]+E[f(Y)g(X)]
$$
If we suppose that $X$ and $Y$ are independent and identically distributed then, as in
this case,
$$
\begin{align*}
E[f(X)g(X)]&=E[f(Y)g(Y)], \\
E[f(X)g(Y)]&=E[f(Y)g(X)]=E[f(X)]E[g(X)]
\end{align*}
$$
we obtain the result when $n=1.$

So assume that @eq-eq11-11 holds for $n-1$ variables, and now suppose that
$X_1,\ldots,X_n$ are independent and $f$ and $g$ are increasing functions. Then
$$
\begin{align*}
&E[f(\mathbf{X})g(\mathbf{X})|X_n = x_n] \\
&=E[f(X_{1},\ldots,X_{n-1},x_{n})g(X_{1},\ldots,X_{n-1},x_{n})|X_{n}=x] \\
&= E[ f( X_1, \ldots , X_{n- 1}, x_n) g( X_1, \ldots , X_{n- 1}, x_n) ] \quad \text{by independence} \\
&\geqslant E[f(X_1,\ldots,X_{n-1},x_n)]E[g(X_1,\ldots,X_{n-1},x_n)] quad \text{by the induction hypothesis} \\
&=E[f(\mathbf{X})|X_{n}=x_{n}]E[g(\mathbf{X})|X_{n}=x_{n}]
\end{align*}
$$
Hence,
$$
E[f(\mathbf{X})g(\mathbf{X})|X_n]\geqslant E[f(\mathbf{X})|X_n]E[g(\mathbf{X})|X_n]
$$
and, upon taking expectations of both sides,
$$
\begin{align*}
E[f(\mathbf{X})g(\mathbf{X})]&\geqslant E\big[E[f(\mathbf{X})|X_{n}]E[g(\mathbf{X})|X_{n}]\big]\\
&\geqslant E[f(\mathbf{X})]E[g(\mathbf{X})]
\end{align*}
$$
The last inequality follows because $E[f(\mathbf{X})|X_n]$ and $E[g(\mathbf{X})|X_n]$ are both increasing functions of $X_n$, and so, by the result for $n=1$,
$$
\begin{align*}
E\Big[E[f(\mathbf{X})|X_{n}]E[g(\mathbf{X})|X_{n}]\Big]&\geqslant E\Big[E[f(\mathbf{X})|X_{n}]\Big]E\Big[E[g(\mathbf{X})|X_{n}]\Big]\\
&=E[f(\mathbf{X})]E[g(\mathbf{X})]\:
\end{align*}
$$
:::

:::{#cor-cor1}
If $U_1,\ldots,U_n$ are independent, and $k$ is either an increasing or decreasing function, then
$$
\mathrm{Cov}\big(k(U_1,\ldots,U_n),k(1-U_1,\ldots,1-U_n)\big)\leqslant0
$$
:::

:::{.proof}
Suppose $k$ is increasing. As $-k(1-U_1,\ldots,1-U_n)$ is increasing in
$U_1,\ldots,U_n$, then, from @thm-thm1,
$$
\mathrm{Cov}\big(k(U_1,\ldots,U_n),-k(1-U_1,\ldots,1-U_n)\big)\geqslant0
$$
When $k$ is decreasing just replace $k$ by its negative. When $k$ is decreasing just replace $k$ by its negative.
:::

Since $F_i^{-1}(U_i)$ is increasing in $U_i$ (as $F_i$, being a distribution function, is increasing) it follows that $g(F_1^{-1}(U_1),\ldots,F_n^{-1}(U_n))$ is a monotone function of $U_{1},\ldots,U_{n}$ whenever $g$ is monotone. Hence, if $g$ is monotone the antithetic variable approach of twice using each set of random numbers $U_1,\ldots,U_n$ by first computing $g(F_1^{-1}(U_1),\ldots,F_n^{-1}(U_n))$ and then $g(F_1^{-1}(1-U_1),\ldots,F_n^{-1}(1-U_n))$ will reduce the variance of the estimate of $E[g(X_1,\ldots,X_n)].$ That is, rather than generating $k$ sets of $n$ random numbers, we should generate $k/2$ sets and use each set twice.

:::{#exm-exa1}
(Simulating the Reliability Function) 
Consider a system of $n$ components in which component $i$, independently of other components, works
with probability $p_i,i=1,\ldots,n.$ Letting
$$
X_i=\begin{cases}1,\quad&\text{if component }i\text{ works}\\
0,\quad&\text{otherwise}
\end{cases}
$$
suppose there is a monotone structure function $\phi$ such that
$$
\phi(X_1,\dots,X_n)=
\begin{cases}
1,&\quad\text{if the system works under }X_1,\dots,X_n\\
0,&\quad\text{otherwise}
\end{cases}
$$
We are interested in using simulation to estimate
$$
r(p_{1},\ldots,p_{n})\equiv E[\phi(X_{1},\ldots,X_{n})]=P\{\phi(X_{1},\ldots,X_{n})=1\}
$$
Now, we can simulate the $X_i$ by generating uniform random numbers $U_1,\ldots,U_n$
and then setting
$$
X_i=
\begin{cases}
1,\quad&\text{if }U_i<p_i\\
0,\quad&\text{otherwise}
\end{cases}
$$
Hence, we see that
$$
\phi(X_1,\ldots,X_n)=k(U_1,\ldots,U_n)
$$
where $k$ is a decreasing function of $U_1,\ldots,U_n.$ Hence,
$$
\mathrm{Cov}(k(\mathbf{U}),k(\mathbf{1}-\mathbf{U}))\leqslant0
$$
and so the antithetic variable approach of using $U_1,\ldots,U_n$ to generate both $k(U_1,\ldots,U_n)$ and $k(1-U_1,\ldots,1-U_n)$ results in a smaller variance than if an independent set of random numbers was used to generate the second $k.$
:::

:::{#exm-exa2}
(Simulating a Queueing System) Consider a given queueing system, and let $D_i$ denote the delay in queue of the $i$th arriving customer, and suppose we are interested in simulating the system so as to estimate
$$
\theta=E[D_1+\cdots+D_n]
$$
Let $X_1,\ldots,X_n$ denote the first $n$ interarrival times and $S_1,\ldots,S_n$ the first $n$ service times of this system, and suppose these random variables are all independent. Now in most systems $D_1+\cdots+D_n$ will be a function of $X_1,\ldots,X_n$, $S_1,\ldots,S_n—$say,
$$
D_1+\cdots+D_n=g(X_1,\ldots,X_n,S_1,\ldots,S_n)
$$
Also $g$ will usually be increasing in $S_i$ and decreasing in $X_i,i=1,\ldots,n.$ If we use the inverse transform method to simulate $X_i,S_i,i=1,\ldots,n$—say, $X_{i}=F_{i}^{-1}(1-U_{i}),S_{i}=G_{i}^{-1}(\bar{U}_{i})$ where $U_1,\ldots,U_{n},\bar{U}_{1},\ldots,\bar{U}_{n}$, are independent uniform random numbers—then we may write
$$
D_1+\cdots+D_n=k(U_1,\ldots,U_n,\bar{U}_1,\ldots,\bar{U}_n)
$$
where $k$ is increasing in its variates. Hence, the antithetic variable approach will reduce the variance of the estimator of $\theta.$ [Thus, we would generate $U_i,\bar{U}_i,i=$ $1,\ldots,n$ and set $X_i=F_i^{-1}(1-U_i)$ and $Y_i=G_i^{-1}(\bar{U}_i)$ for the first run, and $X_{i}=F_{i}^{-1}(U_{i})$ and $Y_i=G_{i}^{-1}(1-\bar{U}_{i})$ for the second.] As all the $U_i$ and $\bar{U}_{i}$ are independent, however, this is equivalent to setting $X_i=F_i^{-1}(U_i),Y_i=G_i^{-1}(\bar{U}_i)$ in the first run and using $1-U_i$ for $U_i$ and $1-\bar{U}_i$ for $\bar{U}_i$ in the second.
:::

:::{#exm-exa3}
Use the crude Monte Carlo and antithetic variable estimators to estimate the standard normal cdf
$$
\Phi(x) = \int_{-\infty}^x\frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt
$$
:::

Since the integration cover an unbounded interval, we break this problem into two cases: $x \ge 0$ and $x < 0$, and use the symmetry of the normal density to handle the second case. So
$$
\Phi(x) = 0.5 + \int_{0}^x\frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt, \, x\ge 0
$$


To estimate $\theta = \int_{0}^{x}e^{-t^2/2}dt$ for $x>0$, we can generate random $U(0,x)$ numbers,  but it would change the parameters of uniform distribution for each different value.

We prefer an algorithm that always samples from $U(0, 1)$ via a change of variables. Making the substitution $y=t/x$, we have $dt=xdy$ and 
$$
\theta = \int_0^1 xe^{-(xy)^2/2}dy
$$
Thus, $\theta=E_Y[xe^{-(xY)^2/2}]$, where $Y\sim U(0,1)$. Finally the Monte Carlo estimator for $\Phi(x)$ is $0.5+\frac{1}{\sqrt{2\pi}}\theta$.

By restricting the simulation to the upper tail, the function $g(\cdot)$ is monotone, so the antithetic variable approach works. Generate random numbers $y_1,\dots, y_{n/2}\sim U(0,1)$ and compute half of the replicates using
\begin{equation*}
g(y_j) = xe^{-(xy_j)^2/2},\,j=1,\dots,n/2
\end{equation*}
and compute the remaining half of the replicates using
\begin{equation*}
g(y_j) = xe^{-(x(1-y_j))^2/2},\,j=1,\dots,n/2
\end{equation*}
Then the antithetic variable estimator is
\begin{equation*}
\theta^{\text{anvar}} = \frac{1}{n}\sum_{j=1}^{n/2}\left(xe^{-(xy_j)^2/2}+xe^{-(x(1-y_j))^2/2}\right)
\end{equation*}

```{python}
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt

nsamples = [2**10, 2**11, 2**12, 2**13, 2**14]
ncopy = 64
l_cmc = np.zeros(len(nsamples))
l_av = np.zeros(len(nsamples))
x = 1

np.random.seed(64)
# Crude Monte Carlo
for i in range(len(nsamples)):
    for j in range(ncopy):
        samples = np.random.rand(nsamples[i])
        l_cmc[i] += np.mean(x*np.exp(-(x*samples)**2/2))
    l_cmc[i] /= ncopy
    l_cmc[i] = 0.5 + l_cmc[i]/np.sqrt(2*np.pi)    

# Antithetic variables
for i in range(len(nsamples)):
    for j in range(ncopy):
        samples = np.random.rand(int(nsamples[i]/2))
        tmp = np.sum(x*np.exp(-(x*samples)**2/2))
        tmp += np.sum(x*np.exp(-(x*(1-samples))**2/2))
        l_av[i] += tmp/nsamples[i]
    l_av[i] /= ncopy
    l_av[i] = 0.5 + l_av[i]/np.sqrt(2*np.pi)    

print('Estimates for crude MC: ', l_cmc)
print('Estimates for antithetic variable estimator: ', l_av)
```

We can see the convergence behavior of the two estimators from the following figure.

```{python}
plt.loglog(nsamples, abs((sp.stats.norm.cdf(1)-l_cmc)/sp.stats.norm.cdf(1)), 'b-', label='crude MC')
plt.loglog(nsamples, abs((sp.stats.norm.cdf(1)-l_av)/sp.stats.norm.cdf(1)), 'r-.', label='antithetic variables')
plt.legend()
plt.xlabel('Sample size')
plt.ylabel('Relative error')
plt.title('$\Phi(1)$ using Monte Carlo methods')
plt.show()
```

